{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Superscript\n",
    "\n",
    "#### Table of content\n",
    "\n",
    "    1 Spark SQL\n",
    "    1.1 Load modules/packages\n",
    "    1.2 Spark connection setup\n",
    "    1.3 Spark Sql selecting/transforming\n",
    "    1.4 Complex Data-types transformation\n",
    "    1.5 User defined functions\n",
    "    1.6 Spark Sql Basic queries\n",
    "    1.7 Spark Sql Advanced 1 queries\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import DataFrame \n",
    "from pyspark.sql import Column \n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql import GroupedData \n",
    "from pyspark.sql import DataFrameNaFunctions       \n",
    "from pyspark.sql import DataFrameStatFunctions    \n",
    "from pyspark.sql import functions \n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import Window \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import from_unixtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Spark connection setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# Create the sc\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Spark SQL selecting/transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|profit|project|\n",
      "+------+-------+\n",
      "|  null|      A|\n",
      "| 12000|      B|\n",
      "| 24000|      C|\n",
      "| 36000|      D|\n",
      "| 48000|      E|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Json file\n",
    "df = spark.read.json(\"C:/big-datademo/superscripts/Python/data/project.json\")\n",
    "# Display thethe DataFrame \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profit: long (nullable = true)\n",
      " |-- project: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema in tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|project|\n",
      "+-------+\n",
      "|      A|\n",
      "|      B|\n",
      "|      C|\n",
      "|      D|\n",
      "|      E|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting \"name\" column\n",
    "df.select(\"project\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|project|(profit + 1)|\n",
      "+-------+------------+\n",
      "|      A|        null|\n",
      "|      B|       12001|\n",
      "|      C|       24001|\n",
      "|      D|       36001|\n",
      "|      E|       48001|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting everybody, but incrementing profit by 1\n",
    "df.select(df['project'], df['profit'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|profit|project|\n",
      "+------+-------+\n",
      "| 48000|      E|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting projects with profit higher than 40000\n",
    "df.filter(df['profit'] > 40000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|profit|count|\n",
      "+------+-----+\n",
      "|  null|    1|\n",
      "| 48000|    1|\n",
      "| 36000|    1|\n",
      "| 24000|    1|\n",
      "| 12000|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count projects by profit\n",
    "df.groupBy(\"profit\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|profit|project|\n",
      "+------+-------+\n",
      "|  null|      A|\n",
      "| 12000|      B|\n",
      "| 24000|      C|\n",
      "| 36000|      D|\n",
      "| 48000|      E|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM project\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|profit|project|\n",
      "+------+-------+\n",
      "|  null|      A|\n",
      "| 12000|      B|\n",
      "| 24000|      C|\n",
      "| 36000|      D|\n",
      "| 48000|      E|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.project\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|profit|project|\n",
      "+------+-------+\n",
      "|  null|      A|\n",
      "| 12000|      B|\n",
      "| 24000|      C|\n",
      "| 36000|      D|\n",
      "| 48000|      E|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.project\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: D\n",
      "project: E\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Loading a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"C:/big-datademo/superscripts/Python/data/project.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "project = parts.map(lambda p: Row(project=p[0], profit=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(project)\n",
    "schemaPeople.createOrReplaceTempView(\"project\")\n",
    "\n",
    "# SQL runned over DataFrames that have been registered as a table.\n",
    "highprofit = spark.sql(\"SELECT project FROM project WHERE profit >= 40000 AND profit <= 100000\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects. Rdd returning the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "projectName = highprofit.rdd.map(lambda p: \"project: \" + p.project).collect()\n",
    "for project in projectName:\n",
    "    print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|project|\n",
      "+-------+\n",
      "|      A|\n",
      "|      B|\n",
      "|      C|\n",
      "|      D|\n",
      "|      E|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Loading a text file and converting each line to a Row.\n",
    "lines = sc.textFile(\"C:/big-datademo/superscripts/Python/data/project.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "project = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"project profit\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Applying the schema to the RDD.\n",
    "schemaProject = spark.createDataFrame(project, schema)\n",
    "\n",
    "# Creating a temporary view using the DataFrame\n",
    "schemaProject.createOrReplaceTempView(\"project\")\n",
    "\n",
    "# SQL runned over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT project FROM project\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"C:/big-datademo/superscripts/Python/data/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"C:/big-datademo/superscripts/Python/data/project.json\", format=\"json\")\n",
    "df.select(\"project\", \"profit\").write.save(\"ProjectProfit.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"C:/big-datademo/superscripts/Python/data/project.csv\",\n",
    "                     format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`C:/big-datademo/superscripts/Python/data/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(42, \"name\").sortBy(\"favorite_numbers\").saveAsTable(\"people_bucketed2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"C:/big-datademo/superscripts/Python/data/users.parquet\")\n",
    "(df\n",
    "    .write\n",
    "    .partitionBy(\"favorite_color\")\n",
    "    .bucketBy(42, \"name\")\n",
    "    .saveAsTable(\"people_partitioned_bucketed1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|project|\n",
      "+-------+\n",
      "|      E|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "projectDF = spark.read.json(\"C:/big-datademo/superscripts/Python/data/project.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "projectDF.write.parquet(\"project.parquet\")\n",
    "\n",
    "# Reading in the Parquet file created above. The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"project.parquet\")\n",
    "\n",
    "# Parquet files used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "highprofit = spark.sql(\"SELECT project FROM parquetFile WHERE profit >= 40000 AND profit <= 100000\")\n",
    "highprofit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Creating a simple DataFrame, stored into a partition directory\n",
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Creating another DataFrame in a new partition directory, adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "\n",
    "# Reading the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- profit: long (nullable = true)\n",
      " |-- project: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|project|\n",
      "+-------+\n",
      "|      C|\n",
      "|      D|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# The JSON dataset is pointed to by path. \n",
    "path = \"C:/big-datademo/superscripts/Python/data/project.json\"\n",
    "projectDF = spark.read.json(path)\n",
    "\n",
    "# The inferred schema visualized using printSchema() \n",
    "projectDF.printSchema()\n",
    "\n",
    "# Creating a temporary view using the DataFrame\n",
    "projectDF.createOrReplaceTempView(\"project\")\n",
    "\n",
    "# SQL statement\n",
    "projectNamesDF = spark.sql(\"SELECT project FROM project WHERE profit BETWEEN 20000 AND 40000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|                Team|project|\n",
      "+--------------------+-------+\n",
      "|[New york, New Yo...|      C|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, a DataFrame is created for a JSON dataset represented by an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"project\":\"C\",\"Team\":{\"city\":\"New York City\",\"State\":\"New york\"}}']\n",
    "otherProjectRDD = sc.parallelize(jsonStrings)\n",
    "otherProject = spark.read.json(otherProjectRDD)\n",
    "otherProject.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|key|  value|\n",
      "+---+-------+\n",
      "|238|val_238|\n",
      "| 86| val_86|\n",
      "|311|val_311|\n",
      "| 27| val_27|\n",
      "|165|val_165|\n",
      "|409|val_409|\n",
      "|255|val_255|\n",
      "|278|val_278|\n",
      "| 98| val_98|\n",
      "|484|val_484|\n",
      "|265|val_265|\n",
      "|193|val_193|\n",
      "|401|val_401|\n",
      "|150|val_150|\n",
      "|273|val_273|\n",
      "|224|val_224|\n",
      "|369|val_369|\n",
      "| 66| val_66|\n",
      "|128|val_128|\n",
      "|213|val_213|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    4500|\n",
      "+--------+\n",
      "\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: None\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 0, Value: val_0\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: None\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 2, Value: val_2\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: None\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 4, Value: val_4\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: None\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 5, Value: val_5\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: None\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 8, Value: val_8\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: None\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "Key: 9, Value: val_9\n",
      "+---+-----+---+-----+\n",
      "|key|value|key|value|\n",
      "+---+-----+---+-----+\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  2|val_2|  2| null|\n",
      "|  2|val_2|  2|val_2|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  4|val_4|  4| null|\n",
      "|  4|val_4|  4|val_4|\n",
      "|  5|val_5|  5|val_5|\n",
      "|  5|val_5|  5|val_5|\n",
      "+---+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# The arehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark as an existing SparkSession\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'C:/big-datademo/superscripts/Python/data/kv1.txt' INTO TABLE src\")\n",
    "\n",
    "# Queries expressed in HiveQL\n",
    "spark.sql(\"SELECT * FROM src\").show()\n",
    "\n",
    "# Aggregation queries \n",
    "spark.sql(\"SELECT COUNT(*) FROM src\").show()\n",
    "\n",
    "# The results of SQL queries; DataFrames supporting all normal functions.\n",
    "sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "\n",
    "# The items in DataFrames are of type Row, allowing access to each column by ordinal.\n",
    "stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "for record in stringsDS.collect():\n",
    "    print(record)\n",
    "    \n",
    "# Using DataFrames to create temporary views within a SparkSession.\n",
    "Record = Row(\"key\", \"value\")\n",
    "recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "recordsDF.createOrReplaceTempView(\"records\")\n",
    "\n",
    "# joining DataFrame data with with queries and data stored in Hive.\n",
    "spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n",
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declaring the function and creating the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "\n",
    "# Creating a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enabling Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generating a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Creating a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Converting the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Complex Data-types transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Function for turning JSON strings into DataFrames.\n",
    "def jsonToDataFrame(json, schema=None):\n",
    "  # SparkSessions are available with Spark 2.0+\n",
    "  reader = spark.read\n",
    "  if schema:\n",
    "    reader.schema(schema)\n",
    "  return reader.json(sc.parallelize([json]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|[1]|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a struct\n",
    "schema = StructType().add(\"a\", StructType().add(\"b\", IntegerType()))\n",
    "                          \n",
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "     \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\", schema)\n",
    "\n",
    "display(events.select(\"a.b\"))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|       a|\n",
      "+--------+\n",
      "|[b -> 1]|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a map\n",
    "schema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n",
    "                          \n",
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "     \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\", schema)\n",
    "\n",
    "display(events.select(\"a.b\"))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: bigint, c: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     a|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "     \"b\": 1,\n",
    "     \"c\": 2\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(\"a.*\"))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: struct<y:bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": 1,\n",
    "  \"b\": 2,\n",
    "  \"c\": 3\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(struct(col(\"a\").alias(\"y\")).alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: struct<a:bigint,b:bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": 1,\n",
    "  \"b\": 2\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(struct(\"*\").alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     a|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": [1, 2]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(col(\"a\").getItem(0).alias(\"x\")))\n",
    "\n",
    "events.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|       a|\n",
      "+--------+\n",
      "|[b -> 1]|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a map\n",
    "schema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n",
    "\n",
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\", schema)\n",
    "\n",
    "display(events.select(col(\"a\").getItem(\"b\").alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     a|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": [1, 2]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(explode(\"a\").alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: string, y: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|               a|\n",
      "+----------------+\n",
      "|[b -> 1, c -> 2]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using a map\n",
    "schema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n",
    "\n",
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1,\n",
    "    \"c\": 2\n",
    "  }\n",
    "}\n",
    "\"\"\", schema)\n",
    "\n",
    "display(events.select(explode(\"a\").alias(\"x\", \"y\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[x: array<bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "[{ \"x\": 1 }, { \"x\": 2 }]\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(collect_list(\"x\").alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[y: string, x: array<bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using an aggregation\n",
    "events = jsonToDataFrame(\"\"\"\n",
    "[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n",
    "\"\"\")\n",
    "\n",
    "display(events.groupBy(\"y\").agg(collect_list(\"x\").alias(\"x\")))\n",
    "\n",
    "events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: array<bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": [\n",
    "    {\"b\": 1},\n",
    "    {\"b\": 2}\n",
    "  ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(\"a.b\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": {\n",
    "    \"b\": 1\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(to_json(\"a\").alias(\"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c: struct<b:int>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": \"{\\\\\"b\\\\\":1}\"\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "schema = StructType().add(\"b\", IntegerType())\n",
    "display(events.select(from_json(\"a\", schema).alias(\"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c: struct<b:struct<x:int,y:string>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": \"{\\\\\"b\\\\\":{\\\\\"x\\\\\":1,\\\\\"y\\\\\":{\\\\\"z\\\\\":2}}}\"\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "schema = StructType().add(\"b\", StructType().add(\"x\", IntegerType())\n",
    "                            .add(\"y\", StringType()))\n",
    "display(events.select(from_json(\"a\", schema).alias(\"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "{\n",
    "  \"a\": \"{\\\\\"b\\\\\":1}\"\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(json_tuple(\"a\", \"b\").alias(\"c\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[c: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = jsonToDataFrame(\"\"\"\n",
    "[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n",
    "\"\"\")\n",
    "\n",
    "display(events.select(regexp_extract(\"a\", \"([a-z]):\", 1).alias(\"c\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(s)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def squared(s):\n",
    "  return s * s\n",
    "sqlContext.udf.register(\"squaredWithPython\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(s)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "def squared_typed(s):\n",
    "  return s * s\n",
    "sqlContext.udf.register(\"squaredWithPython\", squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.range(1, 20).registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * FROM test\") \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|id_squared|\n",
      "+----------+\n",
      "|         1|\n",
      "|         4|\n",
      "|         9|\n",
      "|        16|\n",
      "|        25|\n",
      "|        36|\n",
      "|        49|\n",
      "|        64|\n",
      "|        81|\n",
      "|       100|\n",
      "|       121|\n",
      "|       144|\n",
      "|       169|\n",
      "|       196|\n",
      "|       225|\n",
      "|       256|\n",
      "|       289|\n",
      "|       324|\n",
      "|       361|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select squaredWithPython(id) as id_squared FROM test\") \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, id_squared: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "squared_udf = udf(squared, LongType())\n",
    "df = sqlContext.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Spark Sql basic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|action|      date|\n",
      "+------+----------+\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "df = spark.read.csv(\"C:/big-datademo/superscripts/Python/databricks/export.csv\",\n",
    "                     header=\"true\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|action|      date|\n",
      "+------+----------+\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "|  Open|28/07/2016|\n",
      "| Close|28/07/2016|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registering the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"action\")\n",
    "\n",
    "df = spark.sql(\"select * FROM action\") \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT count(*) FROM action\") \n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT count(*) FROM action\") \n",
    "df.show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------+\n",
      "|      date|action|action_count|\n",
      "+----------+------+------------+\n",
      "|28/07/2016|  Open|         498|\n",
      "|28/07/2016| Close|         502|\n",
      "+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\n",
    "               \n",
    "select date, action, count(action) as action_count from action \n",
    "group by action, date \n",
    "order by date, action desc\") \n",
    "               \n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"C:/big-datademo/superscripts/Python/databricks/Airlines/2008.csv\",\n",
    "                     header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Year='2008', Month='1', DayofMonth='3', DayOfWeek='4', DepTime='2003', CRSDepTime='1955', ArrTime='2211', CRSArrTime='2225', UniqueCarrier='WN', FlightNum='335', TailNum='N712SW', ActualElapsedTime='128', CRSElapsedTime='150', AirTime='116', ArrDelay='-14', DepDelay='8', Origin='IAD', Dest='TPA', Distance='810', TaxiIn='4', TaxiOut='8', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='NA', WeatherDelay='NA', NASDelay='NA', SecurityDelay='NA', LateAircraftDelay='NA')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registering the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"flights\")\n",
    "\n",
    "df = spark.sql(\"select * FROM flights\") \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"DROP TABLE IF EXISTS flights\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql((\"\"\"CREATE TABLE flights\n",
    "USING parquet\n",
    "PARTITIONED BY (Origin)\n",
    "SELECT _c0 as Year, _c1 as Month, _c2 as DayofMonth, _c3 as DayOfWeek, _c4 as DepartureTime, _c5 as CRSDepartureTime, _c6 as ArrivalTime, \n",
    "  _c7 as CRSArrivalTime, _c8 as UniqueCarrier, _c9 as FlightNumber, _c10 as TailNumber, _c11 as ActualElapsedTime, _c12 as CRSElapsedTime, \n",
    "    _c13 as AirTime, _c14 as ArrivalDelay, _c15 as DepartureDelay, _c16 as Origin, _c17 as Destination, _c18 as Distance, \n",
    "    _c19 as TaxiIn, _c20 as TaxiOut, _c21 as Cancelled, _c22 as CancellationCode, _c23 as Diverted, _c24 as CarrierDelay, \n",
    "    _c25 as WeatherDelay, _c26 as NASDelay, _c27 as SecurityDelay, _c28 as LateAircraftDelay \n",
    "FROM csv.`C:/big-datademo/superscripts/Python/databricks/Airlines/2008.csv`\"\"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Year='2008', Month='3', DayofMonth='2', DayOfWeek='7', DepartureTime='1335', CRSDepartureTime='1155', ArrivalTime='1606', CRSArrivalTime='1419', UniqueCarrier='CO', FlightNumber='84', TailNumber='N16642', ActualElapsedTime='151', CRSElapsedTime='144', AirTime='122', ArrivalDelay='107', DepartureDelay='100', Destination='EWR', Distance='745', TaxiIn='8', TaxiOut='21', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='107', SecurityDelay='0', LateAircraftDelay='0', Origin='ATL')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql(\"select * FROM flights\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+\n",
      "|Year|Month|Day|Airduration|\n",
      "+----+-----+---+-----------+\n",
      "|2008|    3|  2|        122|\n",
      "|2008|    5|  9|         81|\n",
      "|2008|    3|  3|        104|\n",
      "|2008|    5| 11|         93|\n",
      "|2008|    3|  4|         94|\n",
      "|2008|    5| 22|         84|\n",
      "|2008|    3|  5|        111|\n",
      "|2008|    5| 13|         82|\n",
      "|2008|    3|  6|         93|\n",
      "|2008|    5|  4|        110|\n",
      "|2008|    3|  7|         90|\n",
      "|2008|    5|  9|         81|\n",
      "|2008|    3|  9|        101|\n",
      "|2008|    5| 14|        108|\n",
      "|2008|    3| 10|        125|\n",
      "|2008|    5|  2|        102|\n",
      "|2008|    3| 11|        108|\n",
      "|2008|    5| 29|         81|\n",
      "|2008|    3| 12|        119|\n",
      "|2008|    5| 18|        109|\n",
      "+----+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "SELECT  Year AS Year, \n",
    "        Month AS Month, \n",
    "        DayofMonth AS Day,\n",
    "        AirTime AS Airduration\n",
    "FROM flights\n",
    "LIMIT 20\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "|Month|Day|Airduration|\n",
      "+-----+---+-----------+\n",
      "|    1|  3|        116|\n",
      "|    1|  3|        113|\n",
      "|    1|  3|         76|\n",
      "|    1|  3|         78|\n",
      "|    1|  3|         77|\n",
      "|    1|  3|         87|\n",
      "|    1|  3|        230|\n",
      "|    1|  3|        219|\n",
      "|    1|  3|         70|\n",
      "|    1|  3|         70|\n",
      "|    1|  3|        106|\n",
      "|    1|  3|        107|\n",
      "|    1|  3|         39|\n",
      "|    1|  3|         37|\n",
      "|    1|  3|         35|\n",
      "|    1|  3|         37|\n",
      "|    1|  3|        213|\n",
      "|    1|  3|        205|\n",
      "|    1|  3|        110|\n",
      "|    1|  3|         49|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ORDER BY \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "SELECT   \n",
    "        Month AS Month, \n",
    "        DayofMonth AS Day,\n",
    "        AirTime AS Airduration\n",
    "FROM flights\n",
    "ORDER BY Month ASC\n",
    "LIMIT 20\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "|Month|Day|Airduration|\n",
      "+-----+---+-----------+\n",
      "|    1|  3|        116|\n",
      "|    1|  3|        113|\n",
      "|    1|  3|         76|\n",
      "|    1|  3|         78|\n",
      "|    1|  3|         77|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WHERE \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "SELECT  Month AS Month, \n",
    "        DayofMonth AS Day,\n",
    "        AirTime AS Airduration\n",
    "FROM flights\n",
    "WHERE Airtime BETWEEN 0 AND 200\n",
    "ORDER BY Month ASC\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\").show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-----------+\n",
      "|Month|Day|Airduration|Destination|\n",
      "+-----+---+-----------+-----------+\n",
      "|    1|  3|        116|        TPA|\n",
      "|    1|  3|        113|        TPA|\n",
      "|    1|  3|         76|        BWI|\n",
      "|    1|  3|         78|        BWI|\n",
      "|    1|  3|         77|        BWI|\n",
      "|    1|  3|         70|        MCI|\n",
      "|    1|  3|         70|        MCI|\n",
      "|    1|  3|        110|        TPA|\n",
      "|    1|  3|         49|        BWI|\n",
      "|    1|  3|         51|        BWI|\n",
      "+-----+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LIKE \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "SELECT  Month AS Month, \n",
    "        DayofMonth AS Day,\n",
    "        AirTime AS Airduration,\n",
    "        Dest AS Destination\n",
    "FROM flights \n",
    "WHERE Dest LIKE 'T%' OR Dest LIKE '%I' AND Airtime BETWEEN 0 AND 300\n",
    "ORDER BY Month ASC\n",
    "LIMIT 10\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------+----+--------+---------+\n",
      "|Month|DayofMonth|CorrectedAirtime|Dest|ArrDelay|InAirTime|\n",
      "+-----+----------+----------------+----+--------+---------+\n",
      "|    1|         3|           104.4| TPA|     -14|    102.0|\n",
      "|    1|         3|           101.7| TPA|       2|    115.0|\n",
      "|    1|         3|           207.0| LAS|      57|    287.0|\n",
      "|    1|         3|           197.1| LAS|     -18|    201.0|\n",
      "|    1|         3|            95.4| MCO|       1|    107.0|\n",
      "+-----+----------+----------------+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculated columns\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT  Month, \n",
    "        DayofMonth,\n",
    "        AirTime * 0.9 AS CorrectedAirtime,\n",
    "        Dest,\n",
    "        ArrDelay,\n",
    "        AirTime+ArrDelay AS InAirTime\n",
    "FROM flights \n",
    "WHERE AirTime > 100\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Month|Airtime|CASE WHEN (CAST(AirTime AS INT) <= 50) THEN Short WHEN (CAST(AirTime AS INT) <= 100) THEN Medium WHEN (CAST(AirTime AS INT) <= 200) THEN long ELSE Very Long END|\n",
      "+-----+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    1|    116|                                                                                                                                                            long|\n",
      "|    1|    113|                                                                                                                                                            long|\n",
      "|    1|     76|                                                                                                                                                          Medium|\n",
      "|    1|     78|                                                                                                                                                          Medium|\n",
      "|    1|     77|                                                                                                                                                          Medium|\n",
      "+-----+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CASE when\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT  Month, Airtime, \n",
    "CASE WHEN AirTime <= 50 THEN 'Short'\n",
    "     WHEN AirTime <= 100 THEN 'Medium'\n",
    "     WHEN AirTime <= 200 THEN 'long'\n",
    "     ELSE 'Very Long' END\n",
    "FROM flights \n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+-------------------------------------------------------------------+\n",
      "|Month|Airtime|Dest|CASE WHEN Dest LIKE A% THEN Premium Flights ELSE Budget Flights END|\n",
      "+-----+-------+----+-------------------------------------------------------------------+\n",
      "|    1|    116| TPA|                                                     Budget Flights|\n",
      "|    1|    113| TPA|                                                     Budget Flights|\n",
      "|    1|     76| BWI|                                                     Budget Flights|\n",
      "|    1|     78| BWI|                                                     Budget Flights|\n",
      "|    1|     77| BWI|                                                     Budget Flights|\n",
      "+-----+-------+----+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CASE when LIKE \n",
    "df = spark.sql(\"\"\"\n",
    "SELECT  Month, Airtime, Dest,\n",
    "CASE WHEN Dest LIKE 'A%' THEN 'Premium Flights'\n",
    "     ELSE 'Budget Flights' END\n",
    "FROM flights \n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+\n",
      "|Month|Origin|TotalFlights|\n",
      "+-----+------+------------+\n",
      "|    6|   ATL|        6046|\n",
      "|    3|   ATL|        6019|\n",
      "|   12|   ATL|        5800|\n",
      "|    9|   ATL|        5722|\n",
      "|    6|   ORD|        5241|\n",
      "|    3|   ORD|        5072|\n",
      "|    9|   ORD|        4931|\n",
      "|    7|   ATL|        4894|\n",
      "|    8|   ATL|        4821|\n",
      "|    4|   ATL|        4798|\n",
      "|   11|   ATL|        4776|\n",
      "|   10|   ATL|        4684|\n",
      "|    5|   ATL|        4656|\n",
      "|    2|   ATL|        4601|\n",
      "|    1|   ATL|        4540|\n",
      "|   12|   ORD|        4473|\n",
      "|    7|   ORD|        4249|\n",
      "|    8|   ORD|        4171|\n",
      "|    4|   ORD|        4140|\n",
      "|    5|   ORD|        4134|\n",
      "+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting top 20 cities with highest monthly total flights on first day of week\n",
    "df = spark.sql(\"\"\"SELECT Month, Origin, count(*) as TotalFlights \n",
    "FROM flights\n",
    "WHERE DayOfWeek = 1 \n",
    "GROUP BY Month, Origin \n",
    "ORDER BY TotalFlights DESC\n",
    "LIMIT 20\"\"\") \n",
    "               \n",
    "df.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Year='2008', Month='1', DayofMonth='3', DayOfWeek='4', DepTime='2003', CRSDepTime='1955', ArrTime='2211', CRSArrTime='2225', UniqueCarrier='WN', FlightNum='335', TailNum='N712SW', ActualElapsedTime='128', CRSElapsedTime='150', AirTime='116', ArrDelay='-14', DepDelay='8', Origin='IAD', Dest='TPA', Distance='810', TaxiIn='4', TaxiOut='8', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='NA', WeatherDelay='NA', NASDelay='NA', SecurityDelay='NA', LateAircraftDelay='NA')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"C:/big-datademo/superscripts/Python/databricks/Airlines/2008.csv\",\n",
    "                     header=\"true\")\n",
    "\n",
    "df.createOrReplaceTempView(\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Year|AirTime|\n",
      "+----+-------+\n",
      "|2008|    122|\n",
      "|2008|     81|\n",
      "|2008|    104|\n",
      "|2008|     93|\n",
      "|2008|     94|\n",
      "|2008|     84|\n",
      "|2008|    111|\n",
      "|2008|     82|\n",
      "|2008|     93|\n",
      "|2008|    110|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select Year,AirTime  from flights limit 10\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    5|        24|        6|    920|       920|    519|      1205|           HA|       21| N580HA|             1379|           345|   1350|    1034|       0|   SEA| HNL|    2677|    16|     13|        0|            null|       0|        1034|           0|       0|            0|                0|\n",
      "|2008|    5|        16|        5|   1021|      1035|    303|      1320|           HA|       29| N592HA|             1182|           345|   1154|     823|     -14|   SEA| OGG|    2640|     5|     23|        0|            null|       0|         823|           0|       0|            0|                0|\n",
      "|2008|    9|         9|        2|   1411|       920|    545|      1215|           HA|       21| N586HA|             1114|           355|   1091|    1050|     291|   SEA| HNL|    2677|     5|     18|        0|            null|       0|        1040|           0|       0|            0|               10|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select * from airlines where Airtime >1000\") \n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|TPA_count|\n",
      "+---------+\n",
      "|    78171|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counting number of rows\n",
    "df = spark.sql(\"\"\"select count(*) as TPA_count from airlines where Dest='TPA'\"\"\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    1|         3|        4|   2003|      1955|   2211|      2225|           WN|      335| N712SW|              128|           150|    116|     -14|       8|   IAD| TPA|     810|     4|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|    754|       735|   1002|      1000|           WN|     3231| N772SW|              128|           145|    113|       2|      19|   IAD| TPA|     810|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    1|         3|        4|   1323|      1255|   1526|      1510|           WN|        4| N674AA|              123|           135|    110|      16|      28|   IND| TPA|     838|     4|      9|        0|            null|       0|           0|           0|       0|            0|               16|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL like\n",
    "df = spark.sql(\"\"\"select * from airlines where Dest like 'T%' Limit 3\"\"\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Year|Month|Dest|\n",
      "+----+-----+----+\n",
      "|2008|    1| TPA|\n",
      "|2008|    1| TPA|\n",
      "|2008|    1| BWI|\n",
      "|2008|    1| BWI|\n",
      "|2008|    1| BWI|\n",
      "|2008|    1| JAX|\n",
      "|2008|    1| LAS|\n",
      "|2008|    1| LAS|\n",
      "|2008|    1| MCI|\n",
      "|2008|    1| MCI|\n",
      "|2008|    1| MCO|\n",
      "|2008|    1| MCO|\n",
      "|2008|    1| MDW|\n",
      "|2008|    1| MDW|\n",
      "|2008|    1| MDW|\n",
      "|2008|    1| MDW|\n",
      "|2008|    1| PHX|\n",
      "|2008|    1| PHX|\n",
      "|2008|    1| TPA|\n",
      "|2008|    1| BWI|\n",
      "+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"select Year,Month,Dest from airlines \"\"\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    6|         2|        1|   1138|      1140|   1224|      1230|           WN|      201| N608SW|               46|            50|     33|      -6|      -2|   TPA| JAX|     180|     4|      9|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    6|         2|        1|   1656|      1645|   1739|      1740|           WN|     1537| N391SW|               43|            55|     32|      -1|      11|   TPA| JAX|     180|     4|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    6|         2|        1|    640|       640|    728|       735|           WN|     2820| N478WN|               48|            55|     34|      -7|       0|   TPA| JAX|     180|     6|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL where with and clause\n",
    "df = spark.sql(\"\"\"select * from airlines where Dest like 'JA%'and (Month = 6 or Month = 12) Limit 3\"\"\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2008|    6|         2|        1|   1956|      1955|   2050|      2055|           WN|       34| N237WN|               54|            60|     45|      -5|       1|   STL| MDW|     251|     2|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    6|         2|        1|   1032|      1030|   1132|      1130|           WN|     1061| N489WN|               60|            60|     45|       2|       2|   STL| MDW|     251|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "|2008|    6|         2|        1|   1401|      1355|   1456|      1455|           WN|     1462| N368SW|               55|            60|     44|       1|       6|   STL| MDW|     251|     5|      6|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL IN clause\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select * from airlines \n",
    "where Month in (6,12)\n",
    "LIMIT 3\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Dest|count|\n",
      "+----+-----+\n",
      "| BGM|  728|\n",
      "| PSE|  753|\n",
      "| DLG|  116|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group By\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select Dest, count(*) as count\n",
    "from airlines \n",
    "group by Dest\n",
    "LIMIT 3\n",
    "\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Dest|count|\n",
      "+----+-----+\n",
      "| BGM|  728|\n",
      "| PSE|  753|\n",
      "| DLG|  116|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group By with having clause\n",
    "\n",
    "df = spark.sql(\"\"\" \n",
    "\n",
    "select Dest, count(*) as count\n",
    "from airlines \n",
    "group by Dest having count > 5\n",
    "LIMIT 3\n",
    "\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|Dest| count|\n",
      "+----+------+\n",
      "| ABE|  4795|\n",
      "| ABI|  2661|\n",
      "| ABQ| 41144|\n",
      "| ABY|  1095|\n",
      "| ACK|   469|\n",
      "| ACT|  1994|\n",
      "| ACV|  3702|\n",
      "| ACY|   113|\n",
      "| ADK|   102|\n",
      "| ADQ|   706|\n",
      "| AEX|  2327|\n",
      "| AGS|  2406|\n",
      "| AKN|   116|\n",
      "| ALB| 13468|\n",
      "| ALO|   323|\n",
      "| AMA|  7490|\n",
      "| ANC| 19329|\n",
      "| ASE|  5243|\n",
      "| ATL|414521|\n",
      "| ATW|  5872|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Order by\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select Dest, count(*) as count\n",
    "from airlines group by Dest having count > 5 order by Dest\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "promoted_content = spark.read.csv(\"C:/big-datademo/superscripts/data/Airline data/promoted_content.csv\",\n",
    "                     header=\"true\")\n",
    "\n",
    "train_clicks = spark.read.csv(\"C:/big-datademo/superscripts/data/Airline data/train_clicks.csv\",\n",
    "                     header=\"true\")\n",
    "\n",
    "events = spark.read.csv(\"C:/big-datademo/superscripts/data/Airline data/events.csv\",\n",
    "                     header=\"true\")\n",
    "\n",
    "promoted_content.createOrReplaceTempView(\"Table_promoted_content\")\n",
    "train_clicks.createOrReplaceTempView(\"Table_train_clicks\")\n",
    "events.createOrReplaceTempView(\"Table_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id|ad_id|clicked|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|   35|      29658|         24|           19|      8157|   35|      0|\n",
      "|  216|      55604|         48|           39|      9646|  216|      1|\n",
      "|  304|      56139|         53|           86|     11384|  304|      0|\n",
      "|  333|      58767|         53|           86|      3738|  333|      0|\n",
      "|  387|      60946|         59|           76|      9292|  387|      0|\n",
      "|  405|      61172|         42|           76|     18092|  405|      0|\n",
      "|  410|      61553|         53|           86|      3738|  410|      0|\n",
      "|  591|      91275|         53|           86|      2957|  591|      0|\n",
      "|  605|    1045711|         90|          138|     14402|  605|      0|\n",
      "|  675|     133963|         53|           86|     10426|  675|      0|\n",
      "|  917|     151471|        142|          241|      8994|  917|      0|\n",
      "|  917|     151471|        142|          241|      3016|  917|      0|\n",
      "|  920|     151014|        127|          138|     12189|  920|      0|\n",
      "|  920|     151014|        127|          138|      6464|  920|      1|\n",
      "| 1144|     174824|         53|           86|      3912| 1144|      0|\n",
      "| 1174|     158484|        166|          150|       375| 1174|      0|\n",
      "| 1181|    1038111|        231|          265|     16911| 1181|      0|\n",
      "| 1222|     688852|         90|          138|      6796| 1222|      0|\n",
      "| 1238|     185913|        114|           67|     13855| 1238|      0|\n",
      "| 1249|     159912|        174|          150|     16495| 1249|      0|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner Join\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select TPC.*,TTC.*\n",
    "from Table_train_clicks TTC\n",
    "inner join Table_promoted_content TPC\n",
    "on TTC.ad_id = TPC.ad_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+------+-------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id| ad_id|clicked|\n",
      "+-----+-----------+-----------+-------------+----------+------+-------+\n",
      "|42337|     938164|       5969|         1499|         1| 42337|      0|\n",
      "| null|       null|       null|         null|         1|139684|      0|\n",
      "| null|       null|       null|         null|         1|144739|      1|\n",
      "| null|       null|       null|         null|         1|156824|      0|\n",
      "| null|       null|       null|         null|         1|279295|      0|\n",
      "| null|       null|       null|         null|         1|296965|      0|\n",
      "| null|       null|       null|         null|         2|125211|      0|\n",
      "| null|       null|       null|         null|         2|156535|      0|\n",
      "| null|       null|       null|         null|         2|169564|      0|\n",
      "| null|       null|       null|         null|         2|308455|      1|\n",
      "|71547|    1043039|       8711|         1919|         3| 71547|      0|\n",
      "|95814|     811706|       3566|         1793|         3| 95814|      0|\n",
      "| null|       null|       null|         null|         3|152141|      0|\n",
      "| null|       null|       null|         null|         3|183846|      0|\n",
      "| null|       null|       null|         null|         3|228657|      1|\n",
      "| null|       null|       null|         null|         3|250082|      0|\n",
      "| null|       null|       null|         null|         4|149930|      0|\n",
      "| null|       null|       null|         null|         4|153623|      1|\n",
      "| null|       null|       null|         null|         4|184709|      0|\n",
      "| null|       null|       null|         null|         4|186849|      0|\n",
      "+-----+-----------+-----------+-------------+----------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Join\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select TPC.*,TTC.*\n",
    "from Table_train_clicks TTC\n",
    "left join Table_promoted_content TPC\n",
    "on TTC.ad_id = TPC.ad_id\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id|ad_id|clicked|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|    1|       6614|          1|            7|      null| null|   null|\n",
      "|    2|     471467|          2|            7|      null| null|   null|\n",
      "|    3|       7692|          3|            7|      null| null|   null|\n",
      "|    4|     471471|          2|            7|      null| null|   null|\n",
      "|    5|     471472|          2|            7|      null| null|   null|\n",
      "|    6|      12736|          1|            7|      null| null|   null|\n",
      "|    7|      12808|          1|            7|      null| null|   null|\n",
      "|    8|     471477|          2|            7|      null| null|   null|\n",
      "|    9|      13379|          1|            7|      null| null|   null|\n",
      "|   10|      13885|          1|            7|      null| null|   null|\n",
      "|   11|      14230|          1|            7|      null| null|   null|\n",
      "|   12|     446701|         10|           19|      null| null|   null|\n",
      "|   13|     471499|         10|           19|      null| null|   null|\n",
      "|   14|     471500|         10|           19|      null| null|   null|\n",
      "|   15|     471501|         10|           19|      null| null|   null|\n",
      "|   16|     471514|         17|           19|      null| null|   null|\n",
      "|   17|     471517|         10|           19|      null| null|   null|\n",
      "|   18|     471518|         10|           19|      null| null|   null|\n",
      "|   19|     471519|          5|           19|      null| null|   null|\n",
      "|   20|     446660|         21|           19|      null| null|   null|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right Join\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select TPC.*,TTC.*\n",
    "from Table_train_clicks TTC\n",
    "right join Table_promoted_content TPC\n",
    "on TTC.ad_id = TPC.ad_id\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|clicked|\n",
      "+-------+\n",
      "|      1|\n",
      "|      1|\n",
      "|      0|\n",
      "|      1|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select clicked FROM Table_train_clicks WHERE ad_id = 144739\n",
    "UNION ALL\n",
    "select advertiser_id FROM Table_promoted_content WHERE ad_id = 144739\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Dest|\n",
      "+----+\n",
      "| BGM|\n",
      "| PSE|\n",
      "| DLG|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distinct\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select distinct Dest from airlines\n",
    "LIMIT 3\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|avg(CAST(Airtime AS DOUBLE))|\n",
      "+----------------------------+\n",
      "|           104.0185891263188|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avg\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select avg(Airtime) from airlines\n",
    "LIMIT 100\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max(Airtime)|\n",
      "+------------+\n",
      "|          NA|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select max(Airtime) from airlines\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|avg(CAST(Airtime AS DOUBLE))|\n",
      "+----------------------------+\n",
      "|           104.0185891263188|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select mean(Airtime) from airlines\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated columns\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT  Month, \n",
    "        DayofMonth,\n",
    "        AirTime * 0.9 AS CorrectedAirtime,\n",
    "        Dest,\n",
    "        ArrDelay,\n",
    "        AirTime+ArrDelay AS InAirTime\n",
    "FROM flights \n",
    "WHERE AirTime > 100\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row(Year='2008', Month='1', DayofMonth='3', DayOfWeek='4', DepTime='2003', \n",
    "    CRSDepTime='1955', ArrTime='2211', CRSArrTime='2225', UniqueCarrier='WN',\n",
    "    FlightNum='335', TailNum='N712SW', ActualElapsedTime='128', CRSElapsedTime='150',\n",
    "    AirTime='116', ArrDelay='-14', DepDelay='8', Origin='IAD', Dest='TPA', \n",
    "    Distance='810', TaxiIn='4', TaxiOut='8', Cancelled='0', CancellationCode=None,\n",
    "    Diverted='0', CarrierDelay='NA', WeatherDelay='NA', NASDelay='NA', SecurityDelay='NA', \n",
    "    LateAircraftDelay='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Month|DayOfMonth|\n",
      "+-----+----------+\n",
      "|    1|         1|\n",
      "|    1|         2|\n",
      "|    1|         3|\n",
      "|    1|         4|\n",
      "|    1|         5|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subqueries\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "Select Month, DayOfMonth\n",
    "FROM flights\n",
    "WHERE Distance =\n",
    "\n",
    "(SELECT MAX(Distance)\n",
    "FROM flights \n",
    ")LIMIT 5\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id|ad_id|clicked|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "|    1|       6614|          1|            7|      null| null|   null|\n",
      "|    2|     471467|          2|            7|      null| null|   null|\n",
      "|    3|       7692|          3|            7|      null| null|   null|\n",
      "|    4|     471471|          2|            7|      null| null|   null|\n",
      "|    5|     471472|          2|            7|      null| null|   null|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correlated subqueries\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select A.*,B.*\n",
    "from Table_train_clicks B\n",
    "right join Table_promoted_content A\n",
    "on B.ad_id = A.ad_id\n",
    "\n",
    "WHERE advertiser_id =\n",
    "\n",
    "(SELECT MAX(advertiser_id)\n",
    "FROM Table_promoted_content AS C\n",
    "WHERE A.ad_id = C.ad_id\n",
    ")LIMIT 5\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Spark Sql Advanced 1 queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+\n",
      "|ad_id|Totalclicked|AvgClicked|\n",
      "+-----+------------+----------+\n",
      "|99876|         0.0|    0.0000|\n",
      "|99819|         0.0|    0.0000|\n",
      "|99803|         0.0|    0.0000|\n",
      "|99800|         0.0|    0.0000|\n",
      "| 9980|         0.0|    0.0000|\n",
      "|99770|         0.0|    0.0000|\n",
      "|99763|         0.0|    0.0000|\n",
      "|99712|         0.0|    0.0000|\n",
      "|99690|         1.0|    0.3333|\n",
      "|99568|         4.0|    0.0625|\n",
      "|99559|         0.0|    0.0000|\n",
      "|99558|         0.0|    0.0000|\n",
      "|99553|         0.0|    0.0000|\n",
      "|99542|        17.0|    0.1954|\n",
      "|99515|        17.0|    0.4359|\n",
      "|99512|        40.0|    0.3738|\n",
      "|99495|         4.0|    0.1905|\n",
      "|99395|         0.0|    0.0000|\n",
      "|99388|         2.0|    0.3333|\n",
      "|99374|         0.0|    0.0000|\n",
      "+-----+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stored procedure query\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select ad_id,\n",
    "SUM(clicked) AS Totalclicked,\n",
    "AVG(CAST(clicked AS DECIMAL)) AS AvgClicked\n",
    "FROM Table_train_clicks\n",
    "GROUP BY ad_id\n",
    "ORDER BY ad_id DESC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Stored procedure \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spClicks\n",
    "AS\n",
    "BEGIN\n",
    "SELECT ad_id,\n",
    "SUM(clicked) AS Totalclicked,\n",
    "AVG(CAST(clicked AS DECIMAL)) AS AvgClicked\n",
    "FROM Table_train_clicks\n",
    "GROUP BY ad_id\n",
    "ORDER BY ad_id DESC\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute stored procedure\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "EXECUTE spClicks\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Alter Stored procedure \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "ALTER PROC spClicks\n",
    "AS\n",
    "BEGIN\n",
    "select ad_id,\n",
    "SUM(clicked) AS Totalclicked,\n",
    "AVG(CAST(clicked AS DECIMAL)) AS AvgClicked\n",
    "FROM Table_train_clicks\n",
    "GROUP BY ad_id\n",
    "ORDER BY ad_id ASC\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Delete Stored procedure  \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DROP PROC spClicks\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+\n",
      "|FlightNum|TotalAirTime|AvgAirTime|\n",
      "+---------+------------+----------+\n",
      "|     9743|        null|      null|\n",
      "|     6917|        null|      null|\n",
      "|     8911|        null|      null|\n",
      "|     8910|        null|      null|\n",
      "|     8941|        null|      null|\n",
      "|     8940|        null|      null|\n",
      "|     9742|        null|      null|\n",
      "|     9002|        19.0|   19.0000|\n",
      "|     7447|        35.0|   35.0000|\n",
      "|     9202|        39.0|   39.0000|\n",
      "|     6918|        45.0|   45.0000|\n",
      "|     6902|        46.0|   46.0000|\n",
      "|     6915|        46.0|   46.0000|\n",
      "|     6272|        46.0|   46.0000|\n",
      "|     6894|        47.0|   47.0000|\n",
      "|     7720|        53.0|   53.0000|\n",
      "|     6870|        60.0|   60.0000|\n",
      "|     5913|        60.0|   60.0000|\n",
      "|     7695|        63.0|   63.0000|\n",
      "|     7476|        71.0|   35.5000|\n",
      "+---------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter query\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select FlightNum, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "GROUP BY FlightNum\n",
    "ORDER BY TotalAirTime ASC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Parameter \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spFlightcriteria(@minAirTime AS INT)\n",
    "AS\n",
    "BEGIN\n",
    "select FlightNum, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "WHERE TotalAirTime > @minAirTime\n",
    "GROUP BY FlightNum\n",
    "ORDER BY TotalAirTime ASC\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Executing SP parameter\n",
    "\n",
    "EXEC spFlightcriteria 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Multiple INT parameters \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spFlightcriteria(@minAirTime AS INT,\n",
    "                             @maxAirTime AS INT)\n",
    "AS\n",
    "BEGIN\n",
    "SELECT FlightNum, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "WHERE TotalAirTime >= @minAirTime AND\n",
    "      TotalAirTime <= @maxAirTime AND\n",
    "GROUP BY FlightNum\n",
    "ORDER BY TotalAirTime ASC\n",
    "END\n",
    "\n",
    "\"\"\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Executing SP multiple INT parameters\n",
    "\n",
    "EXEC spFlightcriteria @minAirTime=300,@maxAirTime=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|Origin|TotalAirTime|AvgAirTime|\n",
      "+------+------------+----------+\n",
      "|   PUB|        40.0|   20.0000|\n",
      "|   TUP|       429.0|   42.9000|\n",
      "|   PIR|       501.0|  100.2000|\n",
      "|   GST|      1268.0|   15.2771|\n",
      "|   BJI|      2701.0|   37.0000|\n",
      "|   INL|      3233.0|   45.5352|\n",
      "|   AKN|      4180.0|   37.3214|\n",
      "|   SUX|      4986.0|   44.9189|\n",
      "|   HTS|      5382.0|   34.7226|\n",
      "|   DLG|      5626.0|   50.6847|\n",
      "|   BPT|      5692.0|   22.1479|\n",
      "|   ITH|      7356.0|   61.3000|\n",
      "|   RHI|      8557.0|   41.3382|\n",
      "|   MKG|      8630.0|   22.1282|\n",
      "|   ALO|      9748.0|   36.3731|\n",
      "|   PLN|     10832.0|   43.3280|\n",
      "|   WRG|     11468.0|   16.7416|\n",
      "|   LWB|     11553.0|   63.1311|\n",
      "|   BLI|     11983.0|  100.6975|\n",
      "|   ACY|     12191.0|  108.8482|\n",
      "+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple INT+VARCHAR parameters query\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select Origin, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "GROUP BY Origin\n",
    "ORDER BY TotalAirTime ASC\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Multiple INT+VARCHAR parameters \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spFlightcriteria(@minAirTime AS INT,\n",
    "                             @maxAirTime AS INT,\n",
    "                             @Origin AS VARCHAR(MAX))\n",
    "AS\n",
    "BEGIN\n",
    "select Origin, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "WHERE TotalAirTime >= @minAirTime AND\n",
    "      TotalAirTime <= maxAirTime AND\n",
    "      Origin LIKE '%' + @Origin + '%'\n",
    "GROUP BY Origin\n",
    "ORDER BY TotalAirTime ASC\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Executing SP multiple INT+VARCHAR parameters \n",
    "\n",
    "EXEC spFlightcriteria @minAirTime=300,@maxAirTime=500,@Origin='AC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Optional parameters\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spFlightcriteria(@minAirTime AS INT = NULL,\n",
    "                             @maxAirTime AS INT = NULL,\n",
    "                             @Origin AS VARCHAR(MAX))\n",
    "AS\n",
    "BEGIN\n",
    "select Origin, \n",
    "SUM(AirTime) AS TotalAirTime,\n",
    "AVG(CAST(AirTime AS DECIMAL)) AS AvgAirTime\n",
    "FROM flights\n",
    "WHERE (@minAirTime = NULL OR TotalAirTime >= @minAirTime) AND\n",
    "      (@maxAirTime = NULL OR TotalAirTime <= @maxAirTime) AND\n",
    "      Origin LIKE '%' + @Origin + '%'\n",
    "GROUP BY Origin\n",
    "ORDER BY TotalAirTime ASC\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Executing SP optional parameters \n",
    "\n",
    "EXEC spFlightcriteria @Origin='AC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id|ad_id|clicked|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "|   35|      29658|         24|           19|      8157|   35|      0|      8157|c260dbb014dccc|    1735810|   563026|       3|   US>OH>535|\n",
      "|  216|      55604|         48|           39|      9646|  216|      1|      9646|d3c5f43efb3e64|    1089240|   666570|       3|   US>AL>630|\n",
      "|  304|      56139|         53|           86|     11384|  304|      0|     11384|610c151b8a2add|    1383758|   790791|       3|       AU>02|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable query. Joining 3 tables to quick view the headings. \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select TPC.*,TTC.*,TE.*\n",
    "from Table_train_clicks TTC\n",
    "inner join Table_promoted_content TPC\n",
    "on TTC.ad_id = TPC.ad_id\n",
    "inner join Table_events TE\n",
    "on TTC.display_id = TE.display_id             \n",
    "\n",
    "              \"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|clicked|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      1|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable query\n",
    "df = spark.sql(\"\"\"\n",
    "select clicked \n",
    "FROM Table_train_clicks \n",
    "WHERE ad_id >= 120000\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select advertiser_id \n",
    "FROM Table_promoted_content \n",
    "WHERE ad_id >= 120000\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select platform \n",
    "FROM Table_events \n",
    "WHERE timestamp >= 800000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Declaring/setting variables\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Time AS INT\n",
    "DECLARE @AD AS INT\n",
    "\n",
    "SET @Time = 800000\n",
    "SET @AD = 120000\n",
    "\n",
    "select clicked \n",
    "FROM Table_train_clicks \n",
    "WHERE ad_id >= @AD\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select advertiser_id \n",
    "FROM Table_promoted_content \n",
    "WHERE ad_id >= @AD\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select platform \n",
    "FROM Table_events \n",
    "WHERE timestamp >= @Time\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|    Number of clicks|scalarsubquery()|\n",
      "+--------------------+----------------+\n",
      "|    Number of clicks|           98162|\n",
      "|Number of promote...|           88082|\n",
      "|    Number of Events|           88478|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable query 2\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Time AS INT\n",
    "DECLARE @NumClicks AS INT\n",
    "DECLARE @NumPromotedContent AS INT\n",
    "DECLARE @NumEvents AS INT\n",
    "\n",
    "SET @Time = >= 100000\n",
    "SET @NumClicks = 12000\n",
    "SET @Events = 12000\n",
    "SET @NumEvents = 800000\n",
    "\n",
    "select 'Number of clicks', @NumClicks\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select 'NumPromotedContent', @NumPromotedContent\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "select 'NumEvents', @NumEvents\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Output parameters\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spOriginInYear  (@Year AS INT,\n",
    "                             @Month AS INT,\n",
    "                             @OriginList AS VARCHAR(MAX) OUTPUT,\n",
    "AS                           @OriginCount AS INT OUTPUT)\n",
    "BEGIN\n",
    "\n",
    "DECLARE @Origin VARCHAR(MAX)\n",
    "SET @Origin = ''\n",
    "\n",
    "SELECT @Origin = @Origin + Origin + ', '\n",
    "FROM flights\n",
    "WHERE Year = @Year AND Month = @Month\n",
    "\n",
    "ORDER BY Origin ASC\n",
    "\n",
    "SET @OriginCount = @@ROWCOUNT \n",
    "SET @OriginList = @Origin\n",
    "\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute the SP OPRV\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Names VARCHAR(MAX)\n",
    "DECLARE @COUNT INT\n",
    "\n",
    "EXEC spFilmsInYear\n",
    "@Year = 2008,\n",
    "@Month = >= 6\n",
    "@OriginList = @Names OUTPUT,\n",
    "@OriginCount = @Count OUTPUT\n",
    "\n",
    "SELECT @Count AS [Number of Origin], @Names AS [List of Origin]\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Return values\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "ALTER PROC spFlightcriteria(@Year AS INT,\n",
    "                            @Month AS INT)\n",
    "AS\n",
    "BEGIN\n",
    "\n",
    "SELECT Origin \n",
    "FROM flights\n",
    "WHERE Year = @Year AND Month = @Month\n",
    "\n",
    "ORDER BY Origin ASC\n",
    "\n",
    "RETURN @@ROWCOUNT\n",
    "\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute SP return values\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Count INT\n",
    "\n",
    "EXEC @COUNT = spOriginInYear @Year = 2008 AND @Month = > 6\n",
    "\n",
    "SELECT @COUNT AS [Number of Origin]\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Stored procedure IF statement 1 \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @IND INT\n",
    "DECLARE @JAX INT\n",
    "\n",
    "SET @IND INT = (SELECT COUNT(*) FROM flights WHERE Origin = IND)\n",
    "SET @JAX INT = (SELECT COUNT(*) FROM flights WHERE Origin = JAX)\n",
    "\n",
    "IF @IND < 8 \n",
    "BEGIN\n",
    "PRINT 'Attention needed'\n",
    "PRINT 'There are too less IND flights'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "PRINT 'Attention needed'\n",
    "PRINT 'There are too many IND flights'\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Stored procedure Nested IF statements \n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @IND INT\n",
    "DECLARE @JAX INT\n",
    "\n",
    "SET @IND INT = (SELECT COUNT(*) FROM flights WHERE Origin = IND)\n",
    "SET @JAX INT = (SELECT COUNT(*) FROM flights WHERE Origin = JAX)\n",
    "\n",
    "IF @IND < 8 \n",
    "BEGIN\n",
    "PRINT 'Attention needed'\n",
    "PRINT 'There are too less IND flights'\n",
    "IF @JAX > 12\n",
    "BEGIN\n",
    "PRINT 'However, The high number of JAX flights compensate enough for the low number of IND flights'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "PRINT 'And The low number of JAX flights do not compensate enough for the low number of IND flights'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "PRINT 'Attention needed'\n",
    "PRINT 'There are too many IND flights'\n",
    "END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Stored procedure IF statement 2\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spVariableData\n",
    "(\n",
    "\n",
    "@InfoType VARCHAR(9)\n",
    "\n",
    ")\n",
    "AS\n",
    "BEGIN\n",
    "\n",
    "        IF @InfoType='ALL'\n",
    "        BEGIN\n",
    "       (SELECT * FROM flights)\n",
    "        RETURN\n",
    "        END\n",
    "        \n",
    "        IF @InfoType='DATE'\n",
    "        BEGIN\n",
    "        (SELECT Year, Month, DayofMonth, DayOfWeek FROM flights)\n",
    "        RETURN\n",
    "        END\n",
    "        \n",
    "        IF @InfoType='TIME'\n",
    "        BEGIN\n",
    "        (SELECT DepTime, CRSDepTime, ArrTime, CRSArrTime, ActualElapsedTime, CRSElapsedTime, AirTime)\n",
    "        RETURN\n",
    "        END\n",
    "        \n",
    "        IF @InfoType='FLIGHTINFO'\n",
    "        BEGIN\n",
    "        (SELECT UniqueCarrier, FlightNum, TailNum, Origin, Dest, Distance)\n",
    "        RETURN\n",
    "        END\n",
    "        \n",
    "        SELECT 'Please choose 'ALL','DATE','TIME' or 'FLIGHTINFO'\n",
    "\n",
    "END\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute SP IF statement 2 \n",
    "\n",
    "EXEC spVariableData @InfoType = 'ALL'\n",
    "EXEC spVariableData @InfoType = 'DATE'\n",
    "EXEC spVariableData @InfoType = 'TIME'\n",
    "EXEC spVariableData @InfoType = 'FLIGHTINFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# WHILE LOOP 1\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Counter INT\n",
    "\n",
    "SET @COUNTER = 1\n",
    "\n",
    "WHILE @COUNTER<=10\n",
    "    BEGIN\n",
    "        PRINT @COUNTER\n",
    "        SET @COUNTER = @COUNTER + 1\n",
    "    END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# WHILE LOOP 2\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Counter INT\n",
    "DECLARE @MaxAirtime INT\n",
    "DECLARE @NumOrigin INT\n",
    "\n",
    "SET @MaxAirtime = (SELECT MAX(AirTime) FROM flights\n",
    "SET @COUNTER = 0\n",
    "\n",
    "WHILE @COUNTER<=@MaxAirtime\n",
    "    BEGIN\n",
    "        SET @NumOrigin=\n",
    "        (SELECT COUNT(*) FROM flights WHERE AirTime = @Counter)\n",
    "        \n",
    "        IF @NumOrigin=0 BREAK\n",
    "        \n",
    "        PRINT CAST(@NumOrigin AS VARCHAR(3)) + 'Origin has flew'\n",
    "            CAST(@COUNTER AS VARCHAR(3)) + '  In AirTime'\n",
    "    END\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# WHILE LOOP 3\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|display_id|ad_id|clicked|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "|   35|      29658|         24|           19|      8157|   35|      0|      8157|c260dbb014dccc|    1735810|   563026|       3|   US>OH>535|\n",
      "+-----+-----------+-----------+-------------+----------+-----+-------+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WHILE LOOP 3. Joining 3 the tables and quick-viewing the headings. \n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "select TPC.*,TTC.*,TE.*\n",
    "from Table_train_clicks TTC\n",
    "inner join Table_promoted_content TPC\n",
    "on TTC.ad_id = TPC.ad_id\n",
    "inner join Table_events TE\n",
    "on TTC.display_id = TE.display_id             \n",
    "\n",
    "              \"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# WHILE LOOP 3.\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Adid INT \n",
    "DECLARE @Platform VARCHAR(MAX)\n",
    "\n",
    "DECLARE AdCursor CURSOR FOR\n",
    "SELECT (SELECT ad_id FROM Table_train_clicks),\n",
    "SELECT (SELECT ad_id FROM Table_events)\n",
    "\n",
    "OPEN AdCursor\n",
    "\n",
    "FETCH NEXT FROM AdCursor INTO @Adid,@Platform\n",
    "\n",
    "WHILE @@FETCH_STATUS = 0\n",
    "    BEGIN\n",
    "        PRINT 'Type of platfom' + @Platform\n",
    "        SELECT Castgeo_location FROM Table_events WHERE Castad_id = @ADid\n",
    "        \n",
    "        FETCH NEXT FROM AdCursor INTO @ADid, @Platform\n",
    "    END\n",
    "    \n",
    "CLOSE AdCursor\n",
    "DEALLOCATE AdCursor\n",
    "\n",
    "\n",
    "\n",
    "              \"\"\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 1\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "EXEC ('SELECT * FROM Table_train_clicks')\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 2\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "EXEC sp_executesql N'SELECT * FROM Table_train_clicks)\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 3\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Tablename NVARCHAR(128)\n",
    "DECLARE @SQLString NVARCHAR(MAX)\n",
    "\n",
    "SET @Tablename = N'Table_train_clicks'\n",
    "\n",
    "SET @SQLString = N'SELECT * FROM' + @Tablename\n",
    "\n",
    "EXEC sp_executesql @SQLString\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 4\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "DECLARE @Number INT\n",
    "DECLARE @NumberString NVARCHAR(4)\n",
    "DECLARE @SQLString NVARCHAR(MAX)\n",
    "\n",
    "SET @Number = 10\n",
    "\n",
    "SET @NumberString = CAST(@Number AS NVARCHAR(4))\n",
    "\n",
    "SET @SQLString = N'SELECT TOP ' + @NumberString + ' * FROM Table_train_clicks ORDER BY ad_id'\n",
    "\n",
    "EXEC sp_executesql @SQLString\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 5 - SP\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "CREATE PROC spVariableTable\n",
    "(\n",
    "\n",
    "        @TableName NVARCHAR(128)\n",
    ")\n",
    "AS\n",
    "BEGIN\n",
    "\n",
    "DECLARE @SQLString NVARCHAR(MAX)\n",
    "\n",
    "SET  @SQLString = N'SELECT * FROM ' + @TableName\n",
    "\n",
    "EXEC sp_executesql @SQLString\n",
    "\n",
    "END\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute dynamic Stored procedure\n",
    "\n",
    "spVariableTable 'Table_train_clicks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Dynamic SQL 5 - SP multiple parameters\n",
    "df = spark.sql(\"\"\"\n",
    "\n",
    "ALTER PROC spVariableTable\n",
    "(\n",
    "\n",
    "        @TableName NVARCHAR(128),\n",
    "        @Number INT\n",
    ")\n",
    "AS\n",
    "BEGIN\n",
    "\n",
    "DECLARE @SQLString NVARCHAR(MAX)\n",
    "DECLARE @NumberString NVARCHAR(4)\n",
    "\n",
    "SET @NumberString = CAST(@Number AS NVARCHAR(4))\n",
    "\n",
    "SET  @SQLString = N'SELECT TOP ' + @NumberString + ' * FROM ' + @TableName\n",
    "\n",
    "EXEC sp_executesql @SQLString\n",
    "\n",
    "END\n",
    "\n",
    "              \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this query in SQL Server Developer.\n",
    "# Execute dynamic Stored procedure multiple parameters\n",
    "\n",
    "spVariableTable 'Table_train_clicks', 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
